## Skin Cancer Classification using ResNet50 (HAM10000 Dataset)

This notebook builds and trains a deep learning model for **skin lesion classification** using the **ResNet50 architecture** pretrained on ImageNet.

**Dataset:** [HAM10000](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000)  
**Classes (7):**
- Melanocytic nevi (nv)
- Melanoma (mel)
- Benign keratosis-like lesions (bkl)
- Basal cell carcinoma (bcc)
- Actinic keratoses (akiec)
- Vascular lesions (vasc)
- Dermatofibroma (df)

## Step 1 — Data Loading and Preprocessing

1. Download HAM10000 dataset using `kagglehub`.
2. Load metadata (`HAM10000_metadata.csv`).
3. Rename columns for clarity and map short diagnosis codes to full names.
4. Fill missing age values with the mean and convert to integer.
5. Encode diagnosis labels (0–6).
6. Check for unmapped values and ensure data consistency.

import kagglehub

# Download latest version
path = kagglehub.dataset_download("kmader/skin-cancer-mnist-ham10000")

print("Path to dataset files:", path)

import os
print("Files in dataset:", os.listdir(path))

import pandas as pd
import os

metadata_path = os.path.join(path, "HAM10000_metadata.csv")
metadata = pd.read_csv(metadata_path)



print(metadata.head())
print(metadata.columns)


metadata.info()

metadata.rename(columns={
    'lesion_id': 'Lesion_ID',
    'image_id': 'Image_ID',
    'dx': 'Diagnosis',
    'dx_type': 'Diagnosis_Type',
    'age': 'Age',
    'sex': 'Sex',
    'localization': 'Localization'
}, inplace=True)


diagnosis_map = {
    'nv': 'Melanocytic nevi (nv)',
    'mel': 'Melanoma (mel)',
    'bkl': 'Benign keratosis-like lesions (bkl)',
    'bcc': 'Basal cell carcinoma (bcc)',
    'akiec': 'Actinic keratoses (akiec)',
    'vasc': 'Vascular lesions (vasc)',
    'df': 'Dermatofibroma (df)'
}

metadata['Diagnosis'] = metadata['Diagnosis'].map(diagnosis_map)


print(metadata.head())


metadata.isnull().sum()

# Handling null values
metadata['Age'].fillna(value=int(metadata['Age'].mean()), inplace=True)
# Converting dtype of age to int32
metadata['Age'] = metadata['Age'].astype('int32')
# Categorically encoding label of the images

label_mapping = {
    'Melanocytic nevi (nv)': 0,
    'Melanoma (mel)': 1,
    'Benign keratosis-like lesions (bkl)': 2,
    'Basal cell carcinoma (bcc)': 3,
    'Actinic keratoses (akiec)': 4,
    'Vascular lesions (vasc)': 5,
    'Dermatofibroma (df)': 6
}
reverse_label_mapping = dict((value, key) for key, value in label_mapping.items())

# Clean the 'Diagnosis' column by stripping whitespace
metadata['Diagnosis'] = metadata['Diagnosis'].str.strip()

metadata['label'] = metadata['Diagnosis'].map(label_mapping)

# Check for any unmapped diagnosis values
unmapped_diagnoses = metadata[metadata['label'].isna()]['Diagnosis'].unique()
if len(unmapped_diagnoses) > 0:
    print("\nWarning: The following diagnosis values were not mapped:", unmapped_diagnoses)

print("\nUnique values in 'Diagnosis' column after cleaning:", metadata['Diagnosis'].unique())
metadata.sample(5)

# Handling null values
metadata['Age'].fillna(value=int(metadata['Age'].mean()), inplace=True)
# Converting dtype of age to int32
metadata['Age'] = metadata['Age'].astype('int32')
# Categorically encoding label of the images
metadata.sample(5)

metadata.info()

print(metadata.head())

print("\nDiagnosis Counts:")
print(metadata['Diagnosis'].value_counts())

import matplotlib.pyplot as plt
metadata['Diagnosis'].value_counts().plot(kind='bar')
plt.title("Distribution of Skin Lesion Types")
plt.xlabel("Diagnosis")
plt.ylabel("Count")
plt.show()


## Step 2 — Handling Class Imbalance

- The HAM10000 dataset is naturally imbalanced.
- Rare classes are **oversampled** by duplicating rows to balance the training set.
- Class distributions are visualized before and after balancing.

df_label1 = metadata[metadata['label'] == 1]
df_label2 = metadata[metadata['label'] == 2]
df_label3 = metadata[metadata['label'] == 3]
df_label4 = metadata[metadata['label'] == 4]
df_label5 = metadata[metadata['label'] == 5]
df_label6 = metadata[metadata['label'] == 6]

df_label1_ovs = pd.concat([df_label1] * 5, ignore_index=True)
df_label2_ovs = pd.concat([df_label2] * 5, ignore_index=True)
df_label3_ovs = pd.concat([df_label3] * 12, ignore_index=True)
df_label4_ovs = pd.concat([df_label4] * 19, ignore_index=True)
df_label5_ovs = pd.concat([df_label5] * 46, ignore_index=True)
df_label6_ovs = pd.concat([df_label6] * 57, ignore_index=True)

final_metadata = pd.concat([metadata,
                        df_label1_ovs,
                        df_label2_ovs,
                        df_label3_ovs,
                        df_label4_ovs,
                        df_label5_ovs,
                        df_label6_ovs], ignore_index=True)

print(final_metadata.head())

print("\nDiagnosis Counts:")
print(final_metadata['Diagnosis'].value_counts())

import matplotlib.pyplot as plt
final_metadata['Diagnosis'].value_counts().plot(kind='bar')
plt.title("Distribution of Skin Lesion Types")
plt.xlabel("Diagnosis")
plt.ylabel("Count")
plt.show()

## Step 3 — Preparing Image Paths and Dataset Splits

- A helper function locates images in the two dataset folders.
- Add an `image_path` column to the metadata.
- Split data into:
  - **Training (64%)**
  - **Validation (16%)**
  - **Testing (20%)**
- Stratified sampling ensures all sets keep the same label distribution.


import os

image_dir1 = os.path.join(path, "HAM10000_images_part_1")
image_dir2 = os.path.join(path, "HAM10000_images_part_2")

def get_image_path(image_id):
    if os.path.exists(os.path.join(image_dir1, image_id + ".jpg")):
        return os.path.join(image_dir1, image_id + ".jpg")
    else:
        return os.path.join(image_dir2, image_id + ".jpg")

final_metadata['image_path'] = final_metadata['Image_ID'].apply(get_image_path)


from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(final_metadata, test_size=0.2, stratify=final_metadata['label'], random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)


## Step 4 — Image Data Generators

- Use `ImageDataGenerator` to:
  - Normalize all images to [0,1] range
  - Apply data augmentation (rotation, shift, horizontal flip) on training images
- Validation and test sets are only rescaled (no augmentation).
- Input size is set to **224×224 pixels** for ResNet50.

from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224  # ResNet50 input size

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    train_df,
    x_col='image_path',
    y_col='Diagnosis',
    target_size=(IMG_SIZE, IMG_SIZE),
    class_mode='categorical',
    batch_size=32
)

val_generator = val_datagen.flow_from_dataframe(
    val_df,
    x_col='image_path',
    y_col='Diagnosis',
    target_size=(IMG_SIZE, IMG_SIZE),
    class_mode='categorical',
    batch_size=32
)

test_generator = test_datagen.flow_from_dataframe(
    test_df,
    x_col='image_path',
    y_col='Diagnosis',
    target_size=(IMG_SIZE, IMG_SIZE),
    class_mode='categorical',
    batch_size=32,
    shuffle=False
)


## Step 5 — Building the ResNet50 Model

- Load **ResNet50 pretrained on ImageNet** (`include_top=False`).
- Unfreeze the **last 30 layers** for fine-tuning.
- Add custom layers:
  - `Flatten → Dense(256, relu) → Dropout(0.5) → Dense(7, softmax)`
- Compile model with:
  - Optimizer: `Adam(lr=1e-4)`
  - Loss: `categorical_crossentropy`
  - Metric: `accuracy`


import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.layers import *

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in base_model.layers[-30:]:
    layer.trainable = True

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(7, activation='softmax')
])


model.summary()

from tensorflow.keras.optimizers import Adam
model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

## Step 6 — Training the Model

- Use callbacks:
  - `EarlyStopping` (monitor validation accuracy, patience=5)
  - `ModelCheckpoint` (save best model automatically)
- Train for **10 epochs** on the augmented training data.
- Save the final model as `skin_cancer_resnet50_finetuned.h5`.

# 5. Callbacks
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

callbacks = [
    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),
    ModelCheckpoint("best_skin_cancer_model.h5", monitor='val_accuracy', save_best_only=True)
]

history = model.fit(train_generator,validation_data=val_generator,epochs=10,callbacks=callbacks)

model.save("skin_cancer_resnet50_finetuned.h5")
print("Model saved successfully!")

## Step 7 — Model Evaluation

- Predict on the test dataset.
- Generate:
  - **Classification report** (precision, recall, F1-score per class)
  - **Confusion matrix** using Seaborn heatmap
  - **Training vs. validation accuracy and loss** plots
- Print **final test accuracy and loss**.


from sklearn.metrics import classification_report
import numpy as np

y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

y_true = test_generator.classes

class_labels = list(test_generator.class_indices.keys())

print(classification_report(y_true, y_pred_classes, target_names=class_labels))


import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Evaluate the model on the test set to get test accuracy and loss
test_loss, test_acc = model.evaluate(test_generator, verbose=0)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Get the class labels
class_labels = list(test_generator.class_indices.keys())

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

## Outputs and Next Steps

**Outputs:**
- Trained model file: `skin_cancer_resnet50_finetuned.h5`
- Best model checkpoint: `best_skin_cancer_model.h5`
- Performance metrics and plots

**Possible Improvements:**
- Use class weights instead of oversampling
- Replace `Flatten()` with `GlobalAveragePooling2D()` to reduce parameters
- Add `ReduceLROnPlateau` to dynamically lower learning rate
- Compute per-class ROC-AUC (more informative for medical tasks)
- Train longer with smaller learning rate for better convergence
